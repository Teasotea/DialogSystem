{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConversationalAI_improved.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "lMzdnYM3AhGM",
        "dpkTWUHtQM8x",
        "rADQQGEOCBhz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teasotea/DialogSystem/blob/main/ConversationalAI_improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "lMzdnYM3AhGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "7xkvwa-_QmLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "x8AfIs3-V_BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEwDK7HzQGzZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "0GzoqEfzWI9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I: Greeting Classification"
      ],
      "metadata": {
        "id": "dpkTWUHtQM8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "greetings = ['hi', \"hola\", 'hey', 'hello','morning', 'evening', 'good day', 'good morning', 'greetings', 'howdy', 'welcome', 'bonjour',\n",
        "             'buenas noches', 'buenos dias', 'salutation', 'salut', 'hail', 'salaam', 'aloha', 'ciao', 'good wishes', 'respects', 'high-five',\n",
        "             'aloha', 'yoo-hoo', 'yawp', 'psst', 'oh', 'toast', 'ave', \"how is it going?\", 'yo', 'hi there']\n",
        "# other = ['face','wisecrack','care','thick','reference','deserve','engine','cry','mud','worth',\n",
        "#          'railroad','permanent','throne','tradition','loan','employ','resource','privilege','parachute',\n",
        "#          'rent','of','characteristic','coin','teenager','established','reveal','bad','undress','revoke','ward']\n",
        "for i in greetings:\n",
        "  training_data.append({\"class\":\"greeting\", \"sentence\":i})\n",
        "# for i in other:\n",
        "#   training_data.append({\"class\":\"other\", \"sentence\":i})\n",
        "training_data.append({\"class\":\"other\", \"sentence\":'word'})\n",
        "  \n",
        "greet_df = pd.DataFrame(training_data)\n",
        "greet_df"
      ],
      "metadata": {
        "id": "09ZwJ8-8QDKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "corpus_words = {}\n",
        "class_words = {}\n",
        "classes = list(set([a['class'] for a in training_data]))\n",
        "for c in classes:\n",
        "    class_words[c] = []"
      ],
      "metadata": {
        "id": "XQvorkvZUX8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "id": "IfOQb7nsWVkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in training_data:\n",
        "    for word in nltk.word_tokenize(data['sentence']):\n",
        "        # ignore a some things\n",
        "        if word not in [\"?\", \"'s\"]:\n",
        "            # stem and lowercase each word\n",
        "            stemmed_word = stemmer.stem(word.lower())\n",
        "            # have we not seen this word already?\n",
        "            if stemmed_word not in corpus_words:\n",
        "                corpus_words[stemmed_word] = 1\n",
        "            else:\n",
        "                corpus_words[stemmed_word] += 1\n",
        "\n",
        "            # add the word to our words in class list\n",
        "            class_words[data['class']].extend([stemmed_word])\n",
        "\n",
        "# we now have each stemmed word and the number of occurances of the word in our training corpus (the word's commonality)\n",
        "print (\"Corpus words and counts: %s \\n\" % corpus_words)\n",
        "# also we have all words in each class\n",
        "print (\"Class words: %s\" % class_words)"
      ],
      "metadata": {
        "id": "6QFC7TMtWb39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate a score for a given class\n",
        "def calculate_class_score(sentence, class_name, show_details=True):\n",
        "    score = 0\n",
        "    # tokenize each word in our new sentence\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "        # check to see if the stem of the word is in any of our classes\n",
        "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
        "            # treat each word with same weight\n",
        "            score += 1\n",
        "            \n",
        "            if show_details:\n",
        "                print (\"   match: %s\" % stemmer.stem(word.lower() ))\n",
        "    return score"
      ],
      "metadata": {
        "id": "S60RV9jpWqmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can now calculate a score for a new sentence\n",
        "sentence = \"good day for us to have lunch?\"\n",
        "\n",
        "# now we can find the class with the highest score\n",
        "for c in class_words.keys():\n",
        "    print (\"Class: %s  Score: %s \\n\" % (c, calculate_class_score(sentence, c)))"
      ],
      "metadata": {
        "id": "6gYPJXNXWvId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate a score for a given class taking into account word commonality\n",
        "def calculate_class_score_commonality(sentence, class_name, show_details=True):\n",
        "    score = 0\n",
        "    # tokenize each word in our new sentence\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "        # check to see if the stem of the word is in any of our classes\n",
        "        if stemmer.stem(word.lower()) in class_words[class_name]:\n",
        "            # treat each word with relative weight\n",
        "            score += (1 / corpus_words[stemmer.stem(word.lower())])\n",
        "\n",
        "            if show_details:\n",
        "                print (\"   match: %s (%s)\" % (stemmer.stem(word.lower()), 1 / corpus_words[stemmer.stem(word.lower())]))\n",
        "    return score"
      ],
      "metadata": {
        "id": "Hzo0LN8bW1_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can find the class with the highest score\n",
        "for c in class_words.keys():\n",
        "    print (\"Class: %s  Score: %s \\n\" % (c, calculate_class_score_commonality(sentence, c)))"
      ],
      "metadata": {
        "id": "jOakDpQCW9C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return the class with highest score for sentence\n",
        "def classify(sentence):\n",
        "    high_class = 'other'\n",
        "    high_score = 0\n",
        "    # loop through our classes\n",
        "    for c in class_words.keys():\n",
        "        # calculate score of sentence for each class\n",
        "        score = calculate_class_score_commonality(sentence, c, show_details=False)\n",
        "        # keep track of highest score\n",
        "        if score > high_score:\n",
        "            high_class = c\n",
        "            high_score = score\n",
        "\n",
        "    return high_class"
      ],
      "metadata": {
        "id": "Io8JCwuEXAds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify(\"oh! are u a human?\")"
      ],
      "metadata": {
        "id": "zUhgtkC2XGGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Question Answering"
      ],
      "metadata": {
        "id": "bqc-tw4XQL2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "RVOiMcK5O1p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "# [ds for ds in datasets.list_datasets() if 'ml' in ds.lower()]"
      ],
      "metadata": {
        "id": "t9b584AubuXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_ds = datasets.load_dataset('squad', streaming = False)\n",
        "qa_ds"
      ],
      "metadata": {
        "id": "lOUBTz1KQOkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_ds['train'].description"
      ],
      "metadata": {
        "id": "fS-kZpaASXwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_ds['train'].dataset_size)\n",
        "qa_ds['train'].features"
      ],
      "metadata": {
        "id": "joklgTTXQono"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_ds['train'].to_pandas().head()"
      ],
      "metadata": {
        "id": "zGit477NSxF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "b_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "cn6UCdC8Up57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_ds['train'] = qa_ds['train'].map(\n",
        "    lambda x: b_tokenizer(\n",
        "        x['question'], x['context'], max_length = 512, padding = 'max_length', truncation = True\n",
        "    ), batched = True, batch_size = 32\n",
        ")"
      ],
      "metadata": {
        "id": "tQ2HXgPHVSJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA Dataset "
      ],
      "metadata": {
        "id": "rADQQGEOCBhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyterlab\n",
        "!pip install python-Levenshtein\n",
        "!pip install bert-serving-server bert-serving-client"
      ],
      "metadata": {
        "id": "7p9XUtm4CLOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_data = pd.read_csv('https://raw.githubusercontent.com/Kizuna-Cheng/Data_Science_Interviews_NLP/main/data.csv')\n",
        "qa_data.head(7)"
      ],
      "metadata": {
        "id": "Sbvz25GUFAXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_data.Questions[:10].tolist()"
      ],
      "metadata": {
        "id": "SNBpTSMQGPM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "             'What does linear regression stands for?',\n",
        " 'What is the differencebetween collinearity and multicollinearity?',\n",
        " 'What are the cons of using a linear model?\\n',\n",
        " 'What are ridge and lasso regression?',\n",
        " 'How does K-Nearest Neighbor work?',\n",
        " 'How to select k for k means?',\n",
        " 'Why is Naive Bayes “naive”?',\n",
        " 'When should I use SVM?',\n",
        "'What is pruning in decision trees?',\n",
        " 'What are random forests? Why is Naive Bayes better?']\n"
      ],
      "metadata": {
        "id": "gL6bNyPYC12O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QA Baseline"
      ],
      "metadata": {
        "id": "MkiqeJfgCbGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getResults(questions, fn):\n",
        "    def getResult(q):\n",
        "        answer, score, prediction = fn(q)\n",
        "        return [q, prediction, answer, score]\n",
        "    return pd.DataFrame(list(map(getResult, questions)), columns=[\"Q\", \"Prediction\", \"A\", \"Score\"])\n",
        "data"
      ],
      "metadata": {
        "id": "gH8zAaMMCMC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getNaiveAnswer(q):\n",
        "    row = qa_data.loc[qa_data['Questions'].str.contains(re.sub(r\"[^\\w'\\s)]+\", \"\", q),case=False)]\n",
        "    if len(row) > 0:\n",
        "        return row[\"Answers\"].values[0], 1, row[\"Questions\"].values[0]\n",
        "    else: return \"Sorry, I didn't get you\", 0, \"\"\n",
        "print(getNaiveAnswer('How does K-Nearest Neighbor work?'))\n",
        "getResults(test_data, getNaiveAnswer)"
      ],
      "metadata": {
        "id": "0AD6kBY_CVXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT model from HuggingFace"
      ],
      "metadata": {
        "id": "qcWVvKXOJ_Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III: Natural Language Generation"
      ],
      "metadata": {
        "id": "L3cyjAcBQDdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint \n",
        "checkpoint = \"microsoft/DialoGPT-medium\"\n",
        "# download and cache tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# download and cache pre-trained model\n",
        "modelNLG = AutoModelForCausalLM.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "pCDZhsH2QhMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV: Chatbot Development"
      ],
      "metadata": {
        "id": "jFnSim68JY7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#change the code later, make it better\n",
        "class ChatBot():\n",
        "    def __init__(self):\n",
        "        # once chat starts, the history will be stored for chat continuity\n",
        "        self.chat_history_ids = None\n",
        "        # make input ids global to use them anywhere within the object\n",
        "        self.bot_input_ids = None\n",
        "        # a flag to check whether to end the conversation\n",
        "        self.end_chat = False\n",
        "        # greet while starting\n",
        "        self.welcome()\n",
        "        self.is_greeting = False\n",
        "        self.is_question_from_context = False\n",
        "        self.answer = ''\n",
        "        \n",
        "    def welcome(self):\n",
        "        print(\"Initializing ChatBot ...\")\n",
        "        # some time to get user ready\n",
        "        time.sleep(2)\n",
        "        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n",
        "        # give time to read what has been printed\n",
        "        time.sleep(3)\n",
        "\n",
        "        \n",
        "    def user_input(self):\n",
        "        # receive input from user\n",
        "        text = input(\"User    >> \")\n",
        "        # end conversation if user wishes so\n",
        "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
        "            # turn flag on \n",
        "            self.end_chat=True\n",
        "            # a closing comment\n",
        "            print('ChatBot >>  See you soon! Bye!')\n",
        "            time.sleep(1)\n",
        "            print('\\nQuitting ChatBot ...')\n",
        "        else:\n",
        "            # continue chat, preprocess input text\n",
        "            # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "            if classify(text) == 'other':\n",
        "              self.answer = getNaiveAnswer(text)[0]\n",
        "              if self.answer != \"Sorry, I didn't get you\":\n",
        "                self.is_question_from_context = True\n",
        "              else: \n",
        "                self.new_user_input_ids = tokenizer.encode(text + tokenizer.eos_token, \\\n",
        "                                                       return_tensors='pt')\n",
        "                self.answer = ''\n",
        "            else: \n",
        "              self.is_greeting = True\n",
        "\n",
        "    def bot_answer(self):\n",
        "      print(\"ChatBot >>  \" + self.answer)\n",
        "      self.is_question_from_context = False\n",
        "      self.answer = ''\n",
        "\n",
        "    def bot_greet(self):\n",
        "        greeting = np.random.choice([\n",
        "            \"Welcome, I am ChatBot, here for your kind service\",\n",
        "            \"Hey, Great day! I am your virtual assistant\",\n",
        "            \"Hello, it's my pleasure meeting you\",\n",
        "            \"Hi, I am a ChatBot. Let's chat!\"\n",
        "        ])\n",
        "        print(\"ChatBot >>  \" + greeting)\n",
        "        self.is_greeting = False\n",
        "\n",
        "    def bot_response(self):\n",
        "        # append the new user input tokens to the chat history\n",
        "        # if chat has already begun\n",
        "        if self.chat_history_ids is not None:\n",
        "            self.bot_input_ids = torch.cat([self.chat_history_ids, self.new_user_input_ids], dim=-1) \n",
        "        else:\n",
        "            # if first entry, initialize bot_input_ids\n",
        "            self.bot_input_ids = self.new_user_input_ids\n",
        "        \n",
        "        # define the new chat_history_ids based on the preceding chats\n",
        "        # generated a response while limiting the total chat history to 1000 tokens, \n",
        "        self.chat_history_ids = modelNLG.generate(self.bot_input_ids, max_length=1000, \\\n",
        "                                               pad_token_id=tokenizer.eos_token_id)\n",
        "            \n",
        "        # last ouput tokens from bot\n",
        "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[-1]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # in case, bot fails to answer\n",
        "        if response == \"\":\n",
        "            response = self.random_response()\n",
        "        # print bot response\n",
        "        print('ChatBot >>  '+ response)\n",
        "        \n",
        "    def random_response(self):\n",
        "        i = -1\n",
        "        response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # iterate over history backwards to find the last token\n",
        "        while response == '':\n",
        "            i = i-1\n",
        "            response = tokenizer.decode(self.chat_history_ids[:, self.bot_input_ids.shape[i]:][0], \\\n",
        "                               skip_special_tokens=True)\n",
        "        # if it is a question, answer suitably\n",
        "        if response.strip() == '?':\n",
        "            reply = np.random.choice([\"I don't know\", \n",
        "                                     \"I am not sure\"])\n",
        "        # not a question? answer suitably\n",
        "        else:\n",
        "            reply = np.random.choice([\"Great\", \n",
        "                                      \"Fine. What's up?\", \n",
        "                                      \"Okay\"\n",
        "                                     ])\n",
        "        return reply"
      ],
      "metadata": {
        "id": "NG4bFpPiQhVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a ChatBot object\n",
        "bot = ChatBot()\n",
        "# start chatting\n",
        "while True:\n",
        "    # receive user input\n",
        "    bot.user_input()\n",
        "    # check whether to end chat\n",
        "    if bot.end_chat:\n",
        "        break\n",
        "    # output bot response\n",
        "    if bot.is_greeting == True:\n",
        "       bot.bot_greet() \n",
        "    elif bot.is_question_from_context == True:\n",
        "       bot.bot_answer()\n",
        "    else: bot.bot_response()   "
      ],
      "metadata": {
        "id": "YjL1zeLuQhip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab73f071-c26c-4987-bdff-f09e1e4a39fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing ChatBot ...\n",
            "Type \"bye\" or \"quit\" or \"exit\" to end chat \n",
            "\n",
            "ChatBot >>  Hello, it's my pleasure meeting you\n",
            "ChatBot >>  Hello, it's my pleasure meeting you\n",
            "ChatBot >>  I would conduct an A/B test to determine if the introduction of a new feature results in a statistically significant improvement in a given metric that we care about. The metric(s) chosen depends on the goal of the feature. For example, a feature may be introduced to increase conversion rates, or web traffic, or retention rates.\n",
            "First I would formulate my null hypothesis (feature X will not improve metric A) and my alternative hypothesis (feature X will improve metric A).\n",
            "Next, I would create my control and test group through random sampling. Because the t-test inherently considers the sample size, I’m not going to specify a necessary sample size, although the larger the better.\n",
            "Once I collect my data, depending on the characteristics of my data, I’d then conduct a t-test, Welch’s t-test, chi-squared test, or a Bayesian A/B test to determine whether the differences between my control and test group are statistically significant.\n"
          ]
        }
      ]
    }
  ]
}